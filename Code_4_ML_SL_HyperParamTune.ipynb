{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Predict the target variable using ML -----------------------\n",
    "# data source URL :\n",
    "# Loan Prediction - https://www.kaggle.com/altruistdelhite04/loan-prediction-problem-dataset\n",
    "\n",
    "# import packages/libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "# handle warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Input Data Collection..................................................................\n",
    "\n",
    "trng_path = \"Input Data/Train_LoanPredictionData.csv\"  # path for the training data sets\n",
    "test_path = \"Input Data/Test_LoanPredictionData.csv\"  # path for the testing data sets\n",
    "\n",
    "# read train data set csv files as a DataFrame\n",
    "trng_df = pd.read_csv(trng_path)\n",
    "trng_df.head()  # explore the first 5 rows\n",
    "\n",
    "# read test data set csv file as a DataFrame\n",
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head()  # explore the first 5 rows\n",
    "print(f\"training set (row, col): {trng_df.shape}\\ntesting set (row, col): {test_df.shape}\")\n",
    "\n",
    "# 2. Input Data Preparation/Cleaning..................................................................\n",
    "\n",
    "# check column information\n",
    "trng_df.head()\n",
    "trng_df.describe()\n",
    "trng_df.info()\n",
    "\n",
    "# dropping ID column for both data sets as all the values are different which doesn't add any value to the model\n",
    "trng_df.drop('Loan_ID', axis=1, inplace=True)\n",
    "test_df.drop('Loan_ID', axis=1, inplace=True)\n",
    "# checking the new shapes\n",
    "print(f\"\\ntraining set (row, col): {trng_df.shape}\\ntesting set (row, col): {test_df.shape}\")\n",
    "\n",
    "# check missing values and print in descending order\n",
    "print(trng_df.isnull().sum().sort_values(ascending=False))\n",
    "\n",
    "# check most frequent value against missing rows for each column\n",
    "print(\"\\ncheck no. of rows for unique value of each column\\n\", \"#\" * 20, \"\\n\")\n",
    "null_cols = trng_df.columns[trng_df.isna().any()].tolist()\n",
    "print(\"Null Columns : \", null_cols)  # print columns having at least a null value\n",
    "# null_cols = ['Credit_History', 'Self_Employed', 'LoanAmount', 'Dependents', 'Loan_Amount_Term', 'Gender', 'Married']\n",
    "for col in null_cols:\n",
    "    print(f\"{col}:\\n{trng_df[col].value_counts()}\\n\", \"-\" * 20)\n",
    "    trng_df[col] = trng_df[col].fillna(trng_df[col].dropna().mode().values[0])\n",
    "\n",
    "# fill the missing value with most frequent value for each column\n",
    "print(\"\\nNo. of rows for each column after filling missing values\\n\", \"#\" * 20, \"\\n\")\n",
    "for col in null_cols:\n",
    "    print(f\"\\n{col}:\\n{trng_df[col].value_counts()}\\n\", \"-\" * 20)  # f is for formatted string\n",
    "\n",
    "# verify that data is still have missing values\n",
    "print(trng_df.isnull().sum().sort_values(ascending=False))\n",
    "print(test_df.isnull().sum().sort_values(ascending=False))\n",
    "print(trng_df.info())\n",
    "print(test_df.info())\n",
    "\n",
    "# extract cleaned data for verification to Output Data directory\n",
    "# converting categorical values to numbers\n",
    "to_numeric = {'3+': 3}\n",
    "# adding the new numeric values from the to_numeric variable to both datasets\n",
    "trng_df = trng_df.applymap(lambda label: to_numeric.get(label) if label in to_numeric else label)\n",
    "test_df = test_df.applymap(lambda label: to_numeric.get(label) if label in to_numeric else label)\n",
    "# converting the Dependents column\n",
    "Dependents_ = pd.to_numeric(trng_df.Dependents)\n",
    "trng_df.to_csv(\"Output Data/7_ML_LoanAmount_CleanedData.csv\")\n",
    "\n",
    "# 3. Feature engineering and visualizing the data to generate insights.............................................\n",
    "\n",
    "# let us visualize the data to find Loan status distribution\n",
    "# split data to categorical and numerical data\n",
    "\n",
    "num = trng_df.select_dtypes('number').columns.to_list()\n",
    "print(num)  # print list of all the numeric columns\n",
    "loan_num = trng_df[num]\n",
    "cat = trng_df.select_dtypes('object').columns.to_list()\n",
    "print(cat)  # print list of all the categorical columns\n",
    "loan_cat = trng_df[cat]\n",
    "\n",
    "# print loan status distribution\n",
    "print(trng_df[cat[-1]].value_counts())\n",
    "\n",
    "# Numerical columns split by histogram\n",
    "for i in loan_num:\n",
    "    plt.hist(loan_num[i])\n",
    "    plt.title(i + ' matplotlib histogram')\n",
    "    plt.show()\n",
    "\n",
    "# Categorical columns split by Loan status:\n",
    "for i in cat[:-1]:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # plt.subplot(2, 3, 1)\n",
    "    sns.countplot(x=i, hue='Loan_Status', data=trng_df)\n",
    "    plt.xlabel(i, fontsize=10)\n",
    "    plt.title('Seaborn Countplot - Loan status distribution by ' + i)\n",
    "    plt.show()\n",
    "\n",
    "# plot seaborn chart - catplot with categorical variables\n",
    "sns.catplot(x=\"Gender\", y=\"LoanAmount\", hue=\"Loan_Status\", col=\"Property_Area\", data=trng_df, kind=\"strip\", height=4,\n",
    "            aspect=.7)\n",
    "# plt.title('catplot matrix')\n",
    "plt.show()\n",
    "\n",
    "# plot seaborn chart - pairplot with categorical variables\n",
    "sns.pairplot(trng_df, vars=['Credit_History', 'ApplicantIncome', 'Loan_Amount_Term'], hue=\"Loan_Status\")\n",
    "plt.title('Seaborn-Pair plot matrix')\n",
    "plt.show()\n",
    "\n",
    "# converting categorical values to numbers\n",
    "to_numeric = {'Male': 1, 'Female': 2, 'Yes': 1, 'No': 2, 'Graduate': 1, 'Not Graduate': 2, 'Urban': 3, 'Semiurban': 2,\n",
    "              'Rural': 1, 'Y': 1, 'N': 0, '3+': 3}\n",
    "\n",
    "# adding the new numeric values from the to_numeric variable to both datasets\n",
    "trng_df = trng_df.applymap(lambda label: to_numeric.get(label) if label in to_numeric else label)\n",
    "test_df = test_df.applymap(lambda label: to_numeric.get(label) if label in to_numeric else label)\n",
    "\n",
    "# converting the Dependents column\n",
    "Dependents_ = pd.to_numeric(trng_df.Dependents)\n",
    "Dependents__ = pd.to_numeric(test_df.Dependents)\n",
    "\n",
    "# dropping the previous Dependents column\n",
    "trng_df.drop(['Dependents'], axis=1, inplace=True)\n",
    "test_df.drop(['Dependents'], axis=1, inplace=True)\n",
    "\n",
    "# concatenation of the new Dependents' column with both datasets\n",
    "trng_df = pd.concat([trng_df, Dependents_], axis=1)\n",
    "test_df = pd.concat([test_df, Dependents__], axis=1)\n",
    "\n",
    "# add total income column as rich feature\n",
    "trng_df['Total_income'] = trng_df['ApplicantIncome']+trng_df['CoapplicantIncome']\n",
    "test_df['Total_income'] = test_df['ApplicantIncome']+test_df['CoapplicantIncome']\n",
    "\n",
    "# checking manipulated dataset for validation\n",
    "print(f\"\\ntraining set (row, col): {trng_df.shape}\\n\\ntesting set (row, col): {test_df.shape}\\n\")\n",
    "print(trng_df.info(), \"\\n\\n\", test_df.info())\n",
    "\n",
    "# extract engineered data for verification\n",
    "trng_df.to_csv(\"Output Data/7_ML_LoanAmount_EngineeredData.csv\")\n",
    "\n",
    "# plotting the correlation matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(trng_df.corr(), cmap='cubehelix_r')\n",
    "plt.show()\n",
    "# correlation table\n",
    "corr = trng_df.corr()\n",
    "print(corr)\n",
    "\n",
    "# Conclusion - We can clearly see that Credit_History has the highest correlation with Loan_Status a positive\n",
    "# correlation 0.540556. Therefore, our target value is highly dependent on the column 'Credit History'.\n",
    "\n",
    "#  4. Execute machine learning algorithm by splitting train and test data...................\n",
    "\n",
    "# Let us divide our dataset into two variables X as the features we defined earlier and y as the Loan_Status the\n",
    "# target value we want to predict.\n",
    "# Models we will use to predict the target value : Random Forest, Decision Tree, XGBoost, Logistic Regression\n",
    "\n",
    "# Create arrays for the features and the response variable\n",
    "y = trng_df['Loan_Status']\n",
    "X = trng_df.drop('Loan_Status', axis=1)\n",
    "# split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# Create ML algorithm score function.........\n",
    "def find_algo_score_fn(algo_fn, algo_name):  # Defines a ml function\n",
    "    ml_model = algo_fn  # # implement random forest ML algorithm\n",
    "    ml_model.fit(X_train, y_train)  # fit the model\n",
    "    y_predict = ml_model.predict(X_test)  # predict the target\n",
    "    print(\"\\n\\n\")\n",
    "    print(classification_report(y_test, y_predict))  # print predicted result\n",
    "    algo_score = accuracy_score(y_predict, y_test)  # find accuracy score\n",
    "    print(f\"{round(algo_score * 100, 2)}% Accurate\")  # print accuracy score\n",
    "    parsed_data = pd.DataFrame({'y_test': y_test, 'prediction': y_predict})\n",
    "    parsed_data.to_csv(\"Output Data/\" + \"7_ML_\" + algo_name + \".csv\")  # print output to csv\n",
    "    return algo_score  # return algorithm score\n",
    "\n",
    "\n",
    "# call algo score function and execute types of ML and find accuracy score\n",
    "RF_SC = find_algo_score_fn(RandomForestClassifier(), \"Random_Forest\")  # implement random forest ML algorithm\n",
    "DT_SC = find_algo_score_fn(DecisionTreeClassifier(), \"Decision_Tree\")  # implement decision tree ML algorithm\n",
    "XGB_SC = find_algo_score_fn(XGBClassifier(), \"XGBoost\")  # implement XGBoost ML algorithm\n",
    "LR_SC = find_algo_score_fn(LogisticRegression(), \"Logistic_Regression\")  # implement random forest ML algorithm\n",
    "\n",
    "# print all ML algorithm's accuracy result\n",
    "score = [round(RF_SC * 100, 2), round(DT_SC * 100, 2), round(XGB_SC * 100, 2), round(LR_SC * 100, 2)]\n",
    "Models = pd.DataFrame({\n",
    "    'ML_algorithm': [\"Random Forest\", \"Decision Tree\", \"XGBoost\", \"Logistic Regression\"],\n",
    "    'Score': score})\n",
    "print(\"\\n\\n\")\n",
    "print(Models.sort_values(by='Score', ascending=False))\n",
    "\n",
    "# cross validation................\n",
    "# Select subset of predictors\n",
    "X = trng_df[['Credit_History', 'Education', 'CoapplicantIncome', 'Self_Employed', 'Dependents', 'ApplicantIncome', 'Married', 'Gender', 'Property_Area', 'LoanAmount', 'Loan_Amount_Term']]\n",
    "y = trng_df['Loan_Status']  # Select target\n",
    "my_pipeline = Pipeline(\n",
    "    steps=[('preprocessor', SimpleImputer()), ('model', RandomForestRegressor(n_estimators=50, random_state=0))])\n",
    "# Multiply by -1 since sklearn calculates *negative* MAE\n",
    "scores = -1 * cross_val_score(my_pipeline, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "# acc_result = cross_val_score(my_pipeline, X, y, cv=5, scoring='accuracy')\n",
    "print(\"\\nMAE scores: \", scores)\n",
    "print(\"\\nAverage MAE score (across experiments): \")\n",
    "print(scores.mean())\n",
    "\n",
    "# print(\"Average accuracy score (across experiments):\")\n",
    "# print(acc_result.mean())\n",
    "\n",
    "\n",
    "#  5. Hyperparameter tuning........................................................................\n",
    "\n",
    "# ...............Hyperparameter tuning with GridSearchCV --------------\n",
    "# Creating the hyper parameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "logreg = LogisticRegression()   # Instantiating logistic regression classifier\n",
    "logreg_cv = GridSearchCV(logreg, param_grid, cv=5)  # Instantiating the GridSearchCV object\n",
    "y = trng_df['Loan_Status']\n",
    "X = trng_df.drop('Loan_Status', axis=1)\n",
    "logreg_cv.fit(X, y)\n",
    "# Print the tuned parameters and score\n",
    "print(\"\\nTuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))\n",
    "\n",
    "# ...............Hyperparameter tuning with RandomizedSearchCV................\n",
    "# Creating the hyperparameter grid\n",
    "param_dist = {\"max_depth\": [3, None], \"max_features\": randint(1, 9), \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "tree = DecisionTreeClassifier()  # Instantiating Decision Tree classifier\n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)    # Instantiating RandomizedSearchCV object\n",
    "# Create arrays for the features and the response variable\n",
    "y = trng_df['Loan_Status'].values\n",
    "X = trng_df.drop('Loan_Status', axis=1).values\n",
    "tree_cv.fit(X, y)\n",
    "# Print the tuned parameters and score\n",
    "print(\"\\nTuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "\n",
    "#  6. Insights ...............................................................................\n",
    "print(\"\\n\\n6.1. Credit_History is most important variable due to its high correlation with Loan_Status.\")\n",
    "print(\"6.2. Most accurate algorithm is 'The Logistic Regression' with approximately 83%.\")\n",
    "print(\"6.3 seaborn chart - catplot - LoanStatus distribution by gender and property area\")\n",
    "print(\"6.4 seaborn chart - pairplot - LoanStatus distribution by credit history, applicant income and loan amount term\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
